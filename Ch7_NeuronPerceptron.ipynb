{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrahmani/ML/blob/main/Ch7_NeuronPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1:** Create a single-neuron **perceptron** with a sign activation function using PyTorch."
      ],
      "metadata": {
        "id": "dV2kUF_un012"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZBRDcxJj4Ns",
        "outputId": "4fd8c958-94c9-4a7b-960e-ca6fe11d506b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: -1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define input features\n",
        "inputs = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# Define weights and bias\n",
        "weights = torch.tensor([-0.5, -0.3, 0.1])\n",
        "bias = torch.tensor(0.2)\n",
        "\n",
        "# Define activation function (in this case, a sign function)\n",
        "def activation(x):\n",
        "    return 1 if x >= 0 else -1\n",
        "\n",
        "# Calculate the weighted sum of inputs and add bias\n",
        "weighted_sum = torch.sum(inputs * weights) + bias\n",
        "\n",
        "# Apply activation function\n",
        "output = activation(weighted_sum)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2:** Create a **Multilayer Perceptron (MLP)** with 2 input neurons, one hidden layer with 3 neurons, and one output neuron using PyTorch."
      ],
      "metadata": {
        "id": "-_dxwf_Jq4yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These lines import the necessary modules from the PyTorch library.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "'''This block of code defines a class called MLP that represents our Multilayer Perceptron model.\n",
        "It inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n",
        "In Python, super() is a built-in function used to call methods of a superclass (or parent class) in a derived class (or subclass).\n",
        "In the __init__ method, we define the layers of our MLP. nn.Linear represents a fully connected layer,\n",
        "where the first parameter is the number of input neurons and the second parameter is the number of output neurons.\n",
        "So, self.hidden represents the hidden layer with 2 input neurons and 3 output neurons,\n",
        "and self.output represents the output layer with 3 input neurons and 1 output neuron.\n",
        "We also initialize the ReLU activation function (nn.ReLU()) and store it in self.activation.\n",
        "ReLU (Rectified Linear Unit) is a commonly used activation function in neural networks.'''\n",
        "\n",
        "'''forward method defines the forward pass of our MLP. It takes an input tensor x and applies the layers sequentially.\n",
        "x = self.activation(self.hidden(x)) performs a forward pass through the hidden layer (self.hidden) followed\n",
        " by the ReLU activation function (self.activation).\n",
        "x = self.output(x) performs a forward pass through the output layer (self.output), which produces the final output of the model.\n",
        "The output tensor x is returned as the result of the forward pass.'''\n",
        "# Define the MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden = nn.Linear(2, 3)  # Hidden layer with 2 input neurons and 3 output neurons\n",
        "        self.output = nn.Linear(3, 1)  # Output layer with 3 input neurons and 1 output neuron\n",
        "        self.activation = nn.ReLU()    # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden(x))  # Forward pass through the hidden layer with ReLU activation\n",
        "        x = self.output(x)                   # Forward pass through the output layer\n",
        "        return x\n",
        "\n",
        "'''This line defines the input data for our MLP as a tensor. In this example, we have a single input data point with two features (2 inputs).'''\n",
        "# Define input tensor\n",
        "inputs = torch.tensor([[1.0, 2.0]])\n",
        "\n",
        "# Create an instance of the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "'''This line performs a forward pass through our MLP model (mlp) with the input data inputs. It calculates the output of the model based on the input data and the weights of the layers.\n",
        "The result is stored in the variable output, '''\n",
        "# Perform a forward pass\n",
        "output = mlp(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA7Yw45Lq7zB",
        "outputId": "6d959a12-7f2a-418c-9f8a-41a19cdb8adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[0.8499]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3:** Create a **Multilayer Perceptron (MLP)** with 2 input neurons, 2 hidden layers (with 3 and 4 neurons respectively), and 2 output neurons using PyTorch."
      ],
      "metadata": {
        "id": "iUGHWpNj42pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden1 = nn.Linear(2, 3)  # First hidden layer with 2 input neurons and 3 output neurons\n",
        "        self.hidden2 = nn.Linear(3, 4)  # Second hidden layer with 3 input neurons and 4 output neurons\n",
        "        self.output = nn.Linear(4, 2)   # Output layer with 4 input neurons and 2 output neurons\n",
        "        self.activation = nn.ReLU()     # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden1(x))  # Forward pass through the first hidden layer with ReLU activation\n",
        "        x = self.activation(self.hidden2(x))  # Forward pass through the second hidden layer with ReLU activation\n",
        "        x = self.output(x)                    # Forward pass through the output layer\n",
        "        return x\n",
        "\n",
        "# Define input tensor\n",
        "inputs = torch.tensor([[1.0, 2.0]])\n",
        "\n",
        "# Create an instance of the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Print weights\n",
        "print(\"Weights of hidden layer 1:\")\n",
        "print(mlp.hidden1.weight)\n",
        "\n",
        "print(\"Weights of hidden layer 2:\")\n",
        "print(mlp.hidden2.weight)\n",
        "\n",
        "print(\"Weights of output layer:\")\n",
        "print(mlp.output.weight)\n",
        "# print(\"Biases of output layer:\")\n",
        "# print(mlp.output.bias)\n",
        "\n",
        "# Perform a forward pass\n",
        "output = mlp(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aKChTVo43WE",
        "outputId": "8aef459e-6f73-4193-cc5b-d758d62f901d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights of hidden layer 1:\n",
            "Parameter containing:\n",
            "tensor([[-0.2032,  0.3708],\n",
            "        [-0.3923, -0.3055],\n",
            "        [ 0.0887, -0.0536]], requires_grad=True)\n",
            "Weights of hidden layer 2:\n",
            "Parameter containing:\n",
            "tensor([[ 0.2755, -0.0055, -0.2226],\n",
            "        [-0.5205,  0.1980,  0.3620],\n",
            "        [-0.5752,  0.4432,  0.3788],\n",
            "        [-0.4613,  0.1508, -0.3776]], requires_grad=True)\n",
            "Weights of output layer:\n",
            "Parameter containing:\n",
            "tensor([[-0.2955, -0.0441, -0.2471, -0.4003],\n",
            "        [-0.0558, -0.2124, -0.2252, -0.4897]], requires_grad=True)\n",
            "Output: tensor([[0.1836, 0.3899]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice:**\n",
        "\n",
        "**Task 1**: In problem 2, find common activation functions available in the PyTorch `torch.nn module`.\n",
        "\n",
        "**Task 2**: In Problem 3, first apply identity activation functions for all layers, and then apply ReLU activation functions for all layers.\n",
        "\n",
        "**Task 3:** In Problem 3, first apply `torch.nn.functional` for **the forward pass** with different activation functions (e.g., `torch.nn.functional.relu()`), then, apply the sign activation function using `torch.sign()`.\n",
        "\n",
        "**Hint:** `torch.nn.functional` is ideal for quick and direct use of activation functions, especially when you don't need to create a custom module. `torch.nn` is more suitable for integrating activation functions into complex neural network architectures and building custom modules."
      ],
      "metadata": {
        "id": "JGhpQ2-dfsIh"
      }
    }
  ]
}